{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ChatPromptBuilder`\n",
    "- Enable GPU in Docker [https://github.com/ollama/ollama/blob/main/docs/docker.md](https://github.com/ollama/ollama/blob/main/docs/docker.md):\n",
    "  ```bash\n",
    "  curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n",
    "      | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\n",
    "  curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n",
    "      | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n",
    "      | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n",
    "  sudo apt-get update\n",
    "\n",
    "  sudo apt-get install -y nvidia-container-toolkit\n",
    "\n",
    "  sudo nvidia-ctk runtime configure --runtime=docker\n",
    "\n",
    "  sudo systemctl restart docker\n",
    "  ```\n",
    "\n",
    "- Run Ollama in Docker integrated with GPU:\n",
    "  ```bash\n",
    "  docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "  docker exec ollama ollama pull hf.co/janhq/Vistral-7b-Chat-GGUF:Q5_K_M\n",
    "  docker exec ollama ollama pull hf.co/janhq/Vistral-7b-Chat-GGUF:Q8_0\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.dataclasses import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = [ChatMessage.from_user(\"Translate to {{ target_language }}. Context: {{ snippet }}; Translation:\")]\n",
    "builder = ChatPromptBuilder(template=template)\n",
    "res = builder.run(target_language=\"spanish\", snippet=\"I can't speak spanish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': [ChatMessage(content=\"Translate to spanish. Context: I can't speak spanish.; Translation:\", role=<ChatRole.USER: 'user'>, name=None, meta={})]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding static template at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = [ChatMessage.from_user(\"Translate to {{ target_language }} and summarize. Context: {{ snippet }}; Summary:\")]\n",
    "res = builder.run(target_language=\"spanish\", snippet=\"I can't speak spanish.\", template=summary_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': [ChatMessage(content=\"Translate to spanish and summarize. Context: I can't speak spanish.; Summary:\", role=<ChatRole.USER: 'user'>, name=None, meta={})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_integrations.components.generators.ollama import OllamaChatGenerator\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no parameter init, we don't use any runtime template variables\n",
    "prompt_builder = ChatPromptBuilder()\n",
    "llm = OllamaChatGenerator(\n",
    "    model=\"hf.co/janhq/Vistral-7b-Chat-GGUF:Q8_0\",\n",
    "    streaming_callback=lambda chunk: print(chunk.content, end=\"\", flush=True),\n",
    "    url = \"http://localhost:11434\",\n",
    "    generation_kwargs={\n",
    "        \"num_predict\": 100,\n",
    "        \"temperature\": 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7058d3937b30>\n",
       "üöÖ Components\n",
       "  - prompt_builder: ChatPromptBuilder\n",
       "  - llm: OllamaChatGenerator\n",
       "üõ§Ô∏è Connections\n",
       "  - prompt_builder.prompt -> llm.messages (List[ChatMessage])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline()\n",
    "pipe.add_component(\"prompt_builder\", prompt_builder)\n",
    "pipe.add_component(\"llm\", llm)\n",
    "pipe.connect(\"prompt_builder.prompt\", \"llm.messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Th√†nh ph·ªë H·ªì Ch√≠ Minh, th∆∞·ªùng ƒë∆∞·ª£c g·ªçi l√† S√†i G√≤n, l√† m·ªôt trong nh·ªØng th√†nh ph·ªë l·ªõn nh·∫•t Vi·ªát Nam. N√≥ n·∫±m ·ªü ph√≠a nam c·ªßa ƒë·∫•t n∆∞·ªõc v√† ƒë√≥ng vai tr√≤ quan tr·ªçng nh∆∞ trung t√¢m th∆∞∆°ng m·∫°i, kinh t·∫ø v√† vƒÉn ho√° ch√≠nh tr·ªã. Th√†nh ph·ªë n·ªïi ti·∫øng v·ªõi nh·ªØng t√≤a nh√† ch·ªçc tr·ªùi hi·ªán ƒë·∫°i c≈©ng nh∆∞ s·ª± pha tr·ªôn gi·ªØa c√°c khu v·ª±c l·ªãch s·ª≠ c·ªï k√≠nh v√† ph√°t tri·ªÉn m·ªõi m·∫ª\n",
      "Th√†nh ph·ªë H·ªì Ch√≠ Minh c√≥ d√¢n s·ªë h∆°n 8 tri·ªáu ng∆∞·ªùi v√† ƒë∆∞·ª£c bi·∫øt ƒë·∫øn{'llm': {'replies': [ChatMessage(content='Th√†nh ph·ªë H·ªì Ch√≠ Minh, th∆∞·ªùng ƒë∆∞·ª£c g·ªçi l√† S√†i G√≤n, l√† m·ªôt trong nh·ªØng th√†nh ph·ªë l·ªõn nh·∫•t Vi·ªát Nam. N√≥ n·∫±m ·ªü ph√≠a nam c·ªßa ƒë·∫•t n∆∞·ªõc v√† ƒë√≥ng vai tr√≤ quan tr·ªçng nh∆∞ trung t√¢m th∆∞∆°ng m·∫°i, kinh t·∫ø v√† vƒÉn ho√° ch√≠nh tr·ªã. Th√†nh ph·ªë n·ªïi ti·∫øng v·ªõi nh·ªØng t√≤a nh√† ch·ªçc tr·ªùi hi·ªán ƒë·∫°i c≈©ng nh∆∞ s·ª± pha tr·ªôn gi·ªØa c√°c khu v·ª±c l·ªãch s·ª≠ c·ªï k√≠nh v√† ph√°t tri·ªÉn m·ªõi m·∫ª\\nTh√†nh ph·ªë H·ªì Ch√≠ Minh c√≥ d√¢n s·ªë h∆°n 8 tri·ªáu ng∆∞·ªùi v√† ƒë∆∞·ª£c bi·∫øt ƒë·∫øn', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={})], 'meta': [{'model': 'hf.co/janhq/Vistral-7b-Chat-GGUF:Q8_0', 'created_at': '2024-12-29T08:21:49.194120862Z', 'done': False, 'done_reason': None, 'total_duration': None, 'load_duration': None, 'prompt_eval_count': None, 'prompt_eval_duration': None, 'eval_count': None, 'eval_duration': None, 'role': 'assistant'}]}}\n"
     ]
    }
   ],
   "source": [
    "location = \"Th√†nh ph·ªë H·ªì Chi Minh\"\n",
    "language = \"Vi·ªát Nam\"\n",
    "system_message = ChatMessage.from_system(\"B·∫°n l√† tr·ª£ l√Ω cung c·∫•p th√¥ng tin cho kh√°ch du l·ªãch t·∫°i {{language}}\")\n",
    "messages = [system_message, ChatMessage.from_user(\"Tell me about {{location}}\")]\n",
    "\n",
    "res = pipe.run(data={\"prompt_builder\": {\"template_variables\": {\"location\": location, \"language\": language},\n",
    "                                    \"template\": messages}})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can ask about the weather in HoChiMinh City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê·ªÉ t√¥i ki·ªÉm tra d·ª± b√°o th·ªùi ti·∫øt cho b·∫°n. Theo d·ª± ƒëo√°n, nhi·ªát ƒë·ªô s·∫Ω ·ªü m·ª©c t·ª´ 26 ƒë·∫øn 31 C v·ªõi ph·∫ßn l·ªõn c√°c ng√†y c√≥ m∆∞a nh·ªè ho·∫∑c kh√¥ng m∆∞a v√†o nh·ªØng ng√†y s·∫Øp t·ªõi. Th·ªùi ti·∫øt ƒë∆∞·ª£c mong ƒë·ª£i l√† n·∫Øng ƒë·∫πp v√† h∆°i ·∫•m √°p. H√£y nh·ªõ mang theo m·ªôt chi·∫øc √¥ n·∫øu c·∫ßn thi·∫øt. T√¥i hy v·ªçng ƒëi·ªÅu n√†y h·ªØu √≠ch!\n",
      "{'llm': {'replies': [ChatMessage(content='ƒê·ªÉ t√¥i ki·ªÉm tra d·ª± b√°o th·ªùi ti·∫øt cho b·∫°n. Theo d·ª± ƒëo√°n, nhi·ªát ƒë·ªô s·∫Ω ·ªü m·ª©c t·ª´ 26 ƒë·∫øn 31 C v·ªõi ph·∫ßn l·ªõn c√°c ng√†y c√≥ m∆∞a nh·ªè ho·∫∑c kh√¥ng m∆∞a v√†o nh·ªØng ng√†y s·∫Øp t·ªõi. Th·ªùi ti·∫øt ƒë∆∞·ª£c mong ƒë·ª£i l√† n·∫Øng ƒë·∫πp v√† h∆°i ·∫•m √°p. H√£y nh·ªõ mang theo m·ªôt chi·∫øc √¥ n·∫øu c·∫ßn thi·∫øt. T√¥i hy v·ªçng ƒëi·ªÅu n√†y h·ªØu √≠ch!\\n', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={})], 'meta': [{'model': 'hf.co/janhq/Vistral-7b-Chat-GGUF:Q8_0', 'created_at': '2024-12-29T08:21:55.576232262Z', 'done': False, 'done_reason': None, 'total_duration': None, 'load_duration': None, 'prompt_eval_count': None, 'prompt_eval_duration': None, 'eval_count': None, 'eval_duration': None, 'role': 'assistant'}]}}\n"
     ]
    }
   ],
   "source": [
    "location = \"Th√†nh ph·ªë H·ªì Chi Minh\"\n",
    "\n",
    "user_message1 = ChatMessage.from_user(\"Th·ªùi ti·∫øt c√πa {{location}} nh∆∞ th·∫ø n√†o trong {{day_count}} ng√†y t·ªõi?\")\n",
    "messages = [system_message, user_message1]\n",
    "res = pipe.run(data={\"prompt_builder\": {\"template_variables\": {\"location\": location, \"day_count\": \"5\"},\n",
    "                                    \"template\": messages}})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√∫ng, c√≥ nhi·ªÅu m√≥n ƒÉn ngon v√† ƒëa d·∫°ng ·ªü th√†nh ph·ªë HCM. M·ªôt s·ªë trong nh·ªØng c√°i ph·ªï bi·∫øn nh·∫•t bao g·ªìm:\n",
      "1. Ph·ªü - m·ªôt b√°t ph·ªü b√≤ ho·∫∑c g√† ·∫•m √°p ƒë∆∞·ª£c ph·ª•c v·ª• v·ªõi m√¨ g·∫°o m·ªèng, n∆∞·ªõc d√πng th∆°m ngon v√† c√°c lo·∫°i th·∫£o m·ªôc t∆∞∆°i.\n",
      "2. B√∫n B√≤ Hu·∫ø - s√∫p ƒë·∫≠m ƒë√† l√†m t·ª´ b√∫n g·∫°o, th·ªãt b√≤, ch·∫£ gi√≤ v√† rau c·ªß ƒÉn k√®m m·∫Øm t√¥m cay.\n",
      "3. C∆°m t·∫•m S√†i G√≤n - c∆°m th∆°m n·∫•u{'llm': {'replies': [ChatMessage(content='ƒê√∫ng, c√≥ nhi·ªÅu m√≥n ƒÉn ngon v√† ƒëa d·∫°ng ·ªü th√†nh ph·ªë HCM. M·ªôt s·ªë trong nh·ªØng c√°i ph·ªï bi·∫øn nh·∫•t bao g·ªìm:\\n1. Ph·ªü - m·ªôt b√°t ph·ªü b√≤ ho·∫∑c g√† ·∫•m √°p ƒë∆∞·ª£c ph·ª•c v·ª• v·ªõi m√¨ g·∫°o m·ªèng, n∆∞·ªõc d√πng th∆°m ngon v√† c√°c lo·∫°i th·∫£o m·ªôc t∆∞∆°i.\\n2. B√∫n B√≤ Hu·∫ø - s√∫p ƒë·∫≠m ƒë√† l√†m t·ª´ b√∫n g·∫°o, th·ªãt b√≤, ch·∫£ gi√≤ v√† rau c·ªß ƒÉn k√®m m·∫Øm t√¥m cay.\\n3. C∆°m t·∫•m S√†i G√≤n - c∆°m th∆°m n·∫•u', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={})], 'meta': [{'model': 'hf.co/janhq/Vistral-7b-Chat-GGUF:Q8_0', 'created_at': '2024-12-29T08:22:01.055821272Z', 'done': False, 'done_reason': None, 'total_duration': None, 'load_duration': None, 'prompt_eval_count': None, 'prompt_eval_duration': None, 'eval_count': None, 'eval_duration': None, 'role': 'assistant'}]}}\n"
     ]
    }
   ],
   "source": [
    "location = \"Th√†nh ph·ªë H·ªì Chi Minh\"\n",
    "\n",
    "user_message2 = ChatMessage.from_user(\"C√≥ m√≥n ƒÉn n√†o l√† ƒë·∫∑c s·∫£n ·ªü {{location}} kh√¥ng?\")\n",
    "messages = [system_message, user_message2]\n",
    "res = pipe.run(data={\"prompt_builder\": {\"template_variables\": {\"location\": location, \"day_count\": \"5\"},\n",
    "                                    \"template\": messages}})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
