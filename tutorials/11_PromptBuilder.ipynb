{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `PromptBuilder`\n",
    "- Enable GPU in Docker [https://github.com/ollama/ollama/blob/main/docs/docker.md](https://github.com/ollama/ollama/blob/main/docs/docker.md):\n",
    "  ```bash\n",
    "  curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n",
    "      | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\n",
    "  curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n",
    "      | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n",
    "      | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n",
    "  sudo apt-get update\n",
    "\n",
    "  sudo apt-get install -y nvidia-container-toolkit\n",
    "\n",
    "  sudo nvidia-ctk runtime configure --runtime=docker\n",
    "\n",
    "  sudo systemctl restart docker\n",
    "  ```\n",
    "\n",
    "- Run Ollama in Docker integrated with GPU:\n",
    "  ```bash\n",
    "  docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "  docker exec ollama ollama pull hf.co/janhq/Vistral-7b-Chat-GGUF:Q5_K_M\n",
    "  docker exec ollama ollama pull hf.co/janhq/Vistral-7b-Chat-GGUF:Q8_0\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.builders import PromptBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Translate the following context to {{ target_language }}. Context: {{ snippet }}; Translation:\"\n",
    "builder = PromptBuilder(template=template)\n",
    "res = builder.run(target_language=\"spanish\", snippet=\"I can't speak spanish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"Translate the following context to spanish. Context: I can't speak spanish.; Translation:\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline, Document\n",
    "from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a real world use case documents could come from a retriever, web, or any other source\n",
    "documents = [Document(content=\"C∆∞·ªùng s·ªëng ·ªü th√†nh ph·ªë H·ªì Ch√≠ Minh\"), Document(content=\"C∆∞·ªùng l√† m·ªôt kƒ© s∆∞ ph·∫ßn m·ªÅm\")]\n",
    "prompt_template = \"\"\"\n",
    "    ƒê∆∞a ra nh·ªØng t√†i li·ªáu n√†y, tr·∫£ l·ªùi c√¢u h·ªèi.\\nT√†i li·ªáu:\n",
    "    {% for doc in documents %}\n",
    "        {{ doc.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    \\nC√¢u h·ªèi: {{query}}\n",
    "    \\nTr·∫£ l·ªùi:\n",
    "    \"\"\"\n",
    "llm = OllamaGenerator(\n",
    "    model=\"hf.co/janhq/Vistral-7b-Chat-GGUF:Q8_0\",\n",
    "    streaming_callback=lambda chunk: print(chunk.content, end=\"\", flush=True),\n",
    "    url = \"http://localhost:11434\",\n",
    "    generation_kwargs={\n",
    "        \"num_predict\": 100,\n",
    "        \"temperature\": 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7913180e5e80>\n",
       "üöÖ Components\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OllamaGenerator\n",
       "üõ§Ô∏è Connections\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Pipeline()\n",
    "p.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "p.add_component(instance=llm, name=\"llm\")\n",
    "p.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr·∫£ l·ªùi: C∆∞·ªùng s·ªëng ·ªü th√†nh ph·ªë H·ªì Ch√≠ Minh. {'llm': {'replies': ['Tr·∫£ l·ªùi: C∆∞·ªùng s·ªëng ·ªü th√†nh ph·ªë H·ªì Ch√≠ Minh. '], 'meta': [{'model': 'hf.co/janhq/Vistral-7b-Chat-GGUF:Q8_0', 'created_at': '2024-12-29T14:24:44.010172429Z', 'done': False, 'done_reason': None, 'total_duration': None, 'load_duration': None, 'prompt_eval_count': None, 'prompt_eval_duration': None, 'eval_count': None, 'eval_duration': None, 'context': None}]}}\n"
     ]
    }
   ],
   "source": [
    "question = \"C∆∞·ªùng s·ªëng ·ªü ƒë√¢u?\"\n",
    "result = p.run({\"prompt_builder\": {\"documents\": documents, \"query\": question}})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the template at runtime (Prompt Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(content=\"C∆∞·ªùng s·ªëng ·ªü th√†nh ph·ªë H·ªì Ch√≠ Minh\", meta={\"name\": \"doc1\"}), \n",
    "    Document(content=\"C∆∞·ªùng l√† m·ªôt kƒ© s∆∞ ph·∫ßn m·ªÅm\", meta={\"name\": \"doc1\"}),\n",
    "]\n",
    "\n",
    "new_template = \"\"\"\n",
    "    B·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªØu √≠ch.\n",
    "    ƒê∆∞a ra nh·ªØng t√†i li·ªáu n√†y, tr·∫£ l·ªùi c√¢u h·ªèi.\n",
    "    T√†i li·ªáu:\n",
    "    {% for doc in documents %}\n",
    "        T√†i li·ªáu {{ loop.index }}:\n",
    "        T√™n t√†i li·ªáu: {{ doc.meta['name'] }}\n",
    "        {{ doc.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    C√¢u h·ªèi: {{ query }}\n",
    "    Tr·∫£ l·ªùi:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C∆∞·ªùng s·ªëng t·∫°i th√†nh ph·ªë H·ªì Ch√≠ Minh.\n",
      "Tr·∫£ l·ªùi:\n",
      "Th√†nh ph·ªë H·ªì Ch√≠ Minh.\n"
     ]
    }
   ],
   "source": [
    "res = p.run({\n",
    "    \"prompt_builder\": {\n",
    "        \"documents\": documents, \n",
    "        \"query\": question, \n",
    "        \"template\": new_template,\n",
    "        },\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overwritting variables at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_template = \"\"\"\n",
    "    B·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªØu √≠ch.\n",
    "    ƒê∆∞a ra nh·ªØng t√†i li·ªáu n√†y, tr·∫£ l·ªùi c√¢u h·ªèi.\n",
    "    T√†i li·ªáu:\n",
    "    {% for doc in documents %}\n",
    "        T√†i li·ªáu {{ loop.index }}:\n",
    "        T√™n t√†i li·ªáu: {{ doc.meta['name'] }}\n",
    "        {{ doc.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    C√¢u h·ªèi: {{ query }}\n",
    "    Vui l√≤ng tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫±ng {{ answer_language | default('Ti·∫øng Vi·ªát') }}\n",
    "    Tr·∫£ l·ªùi:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VƒÉn b·∫£n: T√™n t√†i li·ªáu 1 ghi l√† C∆∞·ªùng s·ªëng ·ªü th√†nh ph·ªë H·ªì Ch√≠ Minh v√† T√†i li·ªáu 2 n√≥i r·∫±ng C∆∞·ªùng l√† m·ªôt kƒ© s∆∞ ph·∫ßn m·ªÅm. C√¢u h·ªèi ƒë·∫∑t ra v·ªÅ vi·ªác C∆∞·ªùng s·ªëng ·ªü ƒë√¢u?\n",
      "    Tr·∫£ l·ªùi: C∆∞·ªùng s·ªëng ·ªü Th√†nh ph·ªë H·ªì Ch√≠ Minh.\n"
     ]
    }
   ],
   "source": [
    "res = p.run({\n",
    "    \"prompt_builder\": {\n",
    "        \"documents\": documents, \n",
    "        \"query\": question, \n",
    "        \"template\": language_template, \n",
    "        \"template_variables\": {\"answer_language\": \"Ti·∫øng Vi·ªát\"},\n",
    "    }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
