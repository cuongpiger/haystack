{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `haystack` modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Install HuggingFace Datasets using \"pip install datasets\"\n",
    "from datasets import load_dataset\n",
    "from haystack import Document, Pipeline\n",
    "from haystack.components.builders.answer_builder import AnswerBuilder\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "# Import LlamaCppChatGenerator\n",
    "from haystack_integrations.components.generators.llama_cpp import LlamaCppChatGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import my custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_file_path = \"./secrets/env\"\n",
    "env_variables = utils.load_env_file(env_file_path=env_file_path)\n",
    "api_key = env_variables[\"OPENAI_SECRET_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `vks.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./data/vks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VKS (VNGCloud Kubernetes Service) is a managed...</td>\n",
       "      <td>What is VKS?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fully Managed control plane: VKS will free you...</td>\n",
       "      <td>Highlights of VKS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When you create a Public Cluster with Public N...</td>\n",
       "      <td>VKS public clusters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When you create a Public Cluster with Public/P...</td>\n",
       "      <td>VKS private clusters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Below is a comparison table between creating a...</td>\n",
       "      <td>Comparison between using Public Cluster and Pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  VKS (VNGCloud Kubernetes Service) is a managed...   \n",
       "1  Fully Managed control plane: VKS will free you...   \n",
       "2  When you create a Public Cluster with Public N...   \n",
       "3  When you create a Public Cluster with Public/P...   \n",
       "4  Below is a comparison table between creating a...   \n",
       "\n",
       "                                               title  \n",
       "0                                       What is VKS?  \n",
       "1                                  Highlights of VKS  \n",
       "2                                VKS public clusters  \n",
       "3                               VKS private clusters  \n",
       "4  Comparison between using Public Cluster and Pr...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is VKS?\n",
      "Highlights of VKS\n",
      "VKS public clusters\n",
      "VKS private clusters\n",
      "Comparison between using Public Cluster and Private Cluster\n"
     ]
    }
   ],
   "source": [
    "for doc in dataset.iloc():\n",
    "    print(doc[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(\n",
    "        content=doc[\"text\"],\n",
    "        meta={\n",
    "            \"title\": doc[\"title\"],\n",
    "        },\n",
    "    )\n",
    "    for doc in dataset.iloc()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: What is VKS?\n",
      "Text: VKS (VNGCloud Kubernetes Service) is a managed service on VNGCloud that simplifies the deployment an...\n",
      "\n",
      "Title: Highlights of VKS\n",
      "Text: Fully Managed control plane: VKS will free you from the burden of managing the Kubernetes Control Pl...\n",
      "\n",
      "Title: VKS public clusters\n",
      "Text: When you create a Public Cluster with Public Node Group , the VKS system will:\n",
      "\n",
      "Create a VM with Flo...\n",
      "\n",
      "Title: VKS private clusters\n",
      "Text: When you create a Public Cluster with Public/Private Node Group , the VKS system will:\n",
      "\n",
      "To enhance t...\n",
      "\n",
      "Title: Comparison between using Public Cluster and Private Cluster\n",
      "Text: Below is a comparison table between creating and using Public Cluster and Private Cluster on the VKS...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in dataset.iloc():\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Text: {doc['text'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DocWriter': {'documents_written': 5}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
    "# Install sentence transformers using \"pip install sentence-transformers\"\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Indexing Pipeline\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(instance=doc_embedder, name=\"DocEmbedder\")\n",
    "indexing_pipeline.add_component(instance=DocumentWriter(document_store=doc_store), name=\"DocWriter\")\n",
    "indexing_pipeline.connect(\"DocEmbedder\", \"DocWriter\")\n",
    "\n",
    "indexing_pipeline.run({\"DocEmbedder\": {\"documents\": docs}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = ChatMessage.from_system(\n",
    "    \"\"\"\n",
    "    Answer the question using the provided context.\n",
    "    Context:\n",
    "    {% for doc in documents %}\n",
    "        {{ doc.content }}\n",
    "    {% endfor %}\n",
    "    \"\"\"\n",
    ")\n",
    "user_message = ChatMessage.from_user(\"Question: {{question}}\")\n",
    "assistent_message = ChatMessage.from_assistant(\"Answer: \")\n",
    "\n",
    "\n",
    "chat_template = [system_message, user_message, assistent_message]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LLAMA model as LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x75abb768ab30>\n",
       "🚅 Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: ChatPromptBuilder\n",
       "  - llm: LlamaCppChatGenerator\n",
       "  - answer_builder: AnswerBuilder\n",
       "🛤️ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - retriever.documents -> answer_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.messages (List[ChatMessage])\n",
       "  - llm.replies -> answer_builder.replies (List[ChatMessage])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_pipeline = Pipeline()\n",
    "\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load the LLM using LlamaCppChatGenerator\n",
    "model_path = \"/mnt/kalista/models/llm/openchat-3.5-1210.Q3_K_S.gguf\"\n",
    "generator = LlamaCppChatGenerator(model=model_path, n_ctx=4096, n_batch=128)\n",
    "\n",
    "rag_pipeline.add_component(\n",
    "    instance=text_embedder,\n",
    "    name=\"text_embedder\",\n",
    ")\n",
    "rag_pipeline.add_component(instance=InMemoryEmbeddingRetriever(document_store=doc_store, top_k=3), name=\"retriever\")\n",
    "rag_pipeline.add_component(instance=ChatPromptBuilder(template=chat_template), name=\"prompt_builder\")\n",
    "rag_pipeline.add_component(instance=generator, name=\"llm\")\n",
    "rag_pipeline.add_component(instance=AnswerBuilder(), name=\"answer_builder\")\n",
    "\n",
    "rag_pipeline.connect(\"text_embedder\", \"retriever\")\n",
    "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "rag_pipeline.connect(\"llm\", \"answer_builder\")\n",
    "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /mnt/kalista/models/llm/openchat-3.5-1210.Q3_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = openchat_openchat-3.5-1210\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 11\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: control token:  32001 '<|pad_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token:  32000 '<|end_of_turn|>' is not marked as EOG\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Small\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 2.95 GiB (3.50 BPW) \n",
      "llm_load_print_meta: general.name     = openchat_openchat-3.5-1210\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|end_of_turn|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 32000 '<|end_of_turn|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q3_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  3017.28 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 4096\n",
      "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
      "llama_new_context_with_model: n_batch       = 128\n",
      "llama_new_context_with_model: n_ubatch      = 128\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    74.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '8192', 'general.name': 'openchat_openchat-3.5-1210', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '11'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{{ 'GPT4 Correct ' + message['role'].title() + ': ' + message['content'] + '<|end_of_turn|>'}}{% endfor %}{% if add_generation_prompt %}{{ 'GPT4 Correct Assistant:' }}{% endif %}\n",
      "Using chat eos_token: <|end_of_turn|>\n",
      "Using chat bos_token: <s>\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 68.63it/s]\n",
      "llama_perf_context_print:        load time =   59596.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   698 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   125 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   90697.15 ms /   823 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VKS, or VNGCloud Kubernetes Service, is a managed service on VNGCloud that simplifies the deployment and management of container-based applications. It is built on top of Kubernetes, an open-source platform developed by Google, and is used to manage and deploy containerized applications in distributed environments. VKS provides a range of features and advantages, such as fully managed control plane, support for the latest Kubernetes versions, efficient and secure networking, seamless upgrades, automatic scaling and healing, reduced costs and enhanced reliability, integration of native blockstore and load balancer, and enhanced security.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is VKS?\"\n",
    "result = rag_pipeline.run(\n",
    "    {\n",
    "        \"text_embedder\": {\"text\": question},\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "        \"llm\": {\"generation_kwargs\": {\"max_tokens\": 128, \"temperature\": 0.1}},\n",
    "        \"answer_builder\": {\"query\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "generated_answer = result[\"answer_builder\"][\"answers\"][0]\n",
    "print(generated_answer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VKS, or VNGCloud Kubernetes Service, is a managed service on VNGCloud that simplifies the deployment and management of container-based applications. It is built on top of Kubernetes, an open-source platform developed by Google, and is used to manage and deploy containerized applications in distributed environments. VKS provides a range of features and advantages, such as fully managed control plane, support for the latest Kubernetes versions, efficient and secure networking, seamless upgrades, automatic scaling and healing, reduced costs and enhanced reliability, integration of native blockstore and load balancer, and enhanced security.\n"
     ]
    }
   ],
   "source": [
    "print(generated_answer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 72.11it/s]\n",
      "Llama.generate: 27 prefix-match hit, remaining 975 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   59596.41 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   975 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  120631.81 ms /  1102 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the VKS system, both Private Clusters and Public Clusters have their own advantages and disadvantages. Here is a comparison between the two:\n",
      "\n",
      "Private Cluster:\n",
      "\n",
      "1. Security: Private Clusters offer higher security as all connections are private and limited access. This ensures strict access control and compliance with security regulations and data privacy.\n",
      "2. Access management: Private Clusters have strict access control, minimizing the risk of external network attacks.\n",
      "3. Scalability: Both Public and Private Clusters are easily scalable through the Auto Scaling feature.\n",
      "4. AutoHealing: Both types of\n"
     ]
    }
   ],
   "source": [
    "question = \"So could you compare between private and public clusters in VKS?\"\n",
    "result = rag_pipeline.run(\n",
    "    {\n",
    "        \"text_embedder\": {\"text\": question},\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "        \"llm\": {\"generation_kwargs\": {\"max_tokens\": 128, \"temperature\": 0.1}},\n",
    "        \"answer_builder\": {\"query\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "generated_answer = result[\"answer_builder\"][\"answers\"][0]\n",
    "print(generated_answer.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
